#' y <- 0
#' x <- rnorm(100)
#' crps(x, y, w=1)
#' crps(x, y, w=0)
#' crps(x, y, w=0.5)
#'
#' @export
crps <- function(y, x, w=0){
M <- length(x)
term1 <- mean(abs(x-y))
if(M <= 6500){
term2 <- sum(outer(x, x, function(a, b) abs(a-b))) # Fastest way for small M
}else{
idx <- unlist(lapply(2:M, function(i) 1:i))
term2 <- 2*sum(abs(x[rep(2:M, 2:M)] - x[idx]))     # Faster for big M
}
res <- term1 + (1 - w/M)/(2*M*(M-1))*term2
}
#' @param w parameter (between 0 and 1). When w=0, the estimator is unbiased ("fair" and "PWM" from Zamo & Naveau 2017). When w=1, the estimator has lower variance ("INT" and "NRG" estimator from Zamo & Naveau 2017).
#' @references Zamo, M., & Naveau, P. (2018). Estimation of the continuous ranked probability score with limited information and applications to ensemble weather forecasts. Mathematical Geosciences, 50(2), 209-234.
#' @examples
#' y <- 0
#' x <- rnorm(100)
#' crps(x, y, w=1)
#' crps(x, y, w=0)
#' crps(x, y, w=0.5)
#'
#' @export
crps <- function(y, x, w=0){
M <- length(x)
term1 <- mean(abs(x-y))
if(M <= 6500){
term2 <- sum(outer(x, x, function(a, b) abs(a-b))) # Fastest way for small M
}else{
idx <- unlist(lapply(2:M, function(i) 1:i))
term2 <- 2*sum(abs(x[rep(2:M, 2:M)] - x[idx]))     # Faster for big M
}
res <- term1 + (1 - w/M)/(2*M*(M-1))*term2
}
curve(F1(x), from=-3, to=3)
curve(F1(x), from=-5, to=5)
curve(F1(x), from=-5, to=5, lwd=2)
curve(I1(x, 0.1), add=TRUE, col='red', lwd=2)
abline(v=0.1)
?integrate
curve(F1(x), from=-5, to=5, lwd=2)
curve(I1(x, 0.1), add=TRUE, col='red', lwd=2, n=1001)
curve(Q(x, 0.1), from=-5, to=5, lwd=2, col='blue')
par(mfrow=c(1,2))
par(mfrow=c(1,2))
x <- rnorm(300)
curve(F1(x), from=-5, to=5, lwd=2)
curve(I1(x, 0.1), add=TRUE, col='red', lwd=2, n=1001)
curve(Q(x, 0.1), from=-5, to=5, lwd=2, col='blue')
integrate(Q, lower=-10, upper=10, y=0.1)
curve(dexp(x)/2, add=TRUE)
curve(dexp(abs(x))/2, add=TRUE)
x <- rnorm(1000)
crps(0.1, x)
print(crps(0.1, x))
M <- length(x)
M
term1 <- mean(abs(x-y))
if(M <= 6500){
term2 <- sum(outer(x, x, function(a, b) abs(a-b))) # Fastest way for small M
}else{
idx <- unlist(lapply(2:M, function(i) 1:i))
term2 <- 2*sum(abs(x[rep(2:M, 2:M)] - x[idx]))     # Faster for big M
}
term2
term1
term1
term1 <- mean(abs(x-y))
y=0.1
term1 <- mean(abs(x-y))
term1
term2/(2*M*(M-1))
term1 + term2/(2*M*(M-1))
#' @param w parameter (between 0 and 1). When w=0, the estimator is unbiased ("fair" and "PWM" from Zamo & Naveau 2017). When w=1, the estimator has lower variance ("INT" and "NRG" estimator from Zamo & Naveau 2017).
#' @references Zamo, M., & Naveau, P. (2018). Estimation of the continuous ranked probability score with limited information and applications to ensemble weather forecasts. Mathematical Geosciences, 50(2), 209-234.
#' @examples
#' y <- 0
#' x <- rnorm(100)
#' crps(x, y, w=1)
#' crps(x, y, w=0)
#' crps(x, y, w=0.5)
#'
#' @export
crps <- function(y, x, w=0){
M <- length(x)
term1 <- mean(abs(x-y))
if(M <= 6500){
term2 <- sum(outer(x, x, function(a, b) abs(a-b))) # Fastest way for small M
}else{
idx <- unlist(lapply(2:M, function(i) 1:i))
term2 <- 2*sum(abs(x[rep(2:M, 2:M)] - x[idx]))     # Faster for big M
}
res <- term1 - (1 - w/M)*term2/(2*M*(M-1))
}
print(crps(0.1, x))
#' @param w parameter (between 0 and 1). When w=0, the estimator is unbiased ("fair" and "PWM" from Zamo & Naveau 2017). When w=1, the estimator has lower variance ("INT" and "NRG" estimator from Zamo & Naveau 2017).
#' @references Zamo, M., & Naveau, P. (2018). Estimation of the continuous ranked probability score with limited information and applications to ensemble weather forecasts. Mathematical Geosciences, 50(2), 209-234.
#' @examples
#' y <- 0
#' x <- rnorm(100)
#' crps(x, y, w=1)
#' crps(x, y, w=0)
#' crps(x, y, w=0.5)
#'
#' @export
crps <- function(y, x, w=0){
M <- length(x)
term1 <- mean(abs(x-y))
if(M <= 6500){
term2 <- sum(outer(x, x, function(a, b) abs(a-b))) # Fastest way for small M
}else{
idx <- unlist(lapply(2:M, function(i) 1:i))
term2 <- 2*sum(abs(x[rep(2:M, 2:M)] - x[idx]))     # Faster for big M
}
res <- term1 - (1 - w/M)*term2/(2*M*(M-1))
return(res)
}
crps(0.1, x)
integrate(Q, lower=-10, upper=10, y=0.1)
integrate(Q, lower=-10, upper=10, y=0.1)$value
F1 <- function(x) pnorm(x)
I1 <- function(x, y) as.numeric(x >= y)
Q <- function(x, y) (F1(x) - I1(x, y))^2
y <- 0.1
x <- rnorm(1000)
crps(y, x)
integrate(Q, lower=-10, upper=10, y=0.1)$value
x <- rnorm(100)
crps(y, x)
integrate(Q, lower=-10, upper=10, y=0.1)$value
document()
document()
?crps
F1 <- function(x) pnorm(x)
I1 <- function(x, y) as.numeric(x >= y)
Q <- function(x, y) (F1(x) - I1(x, y))^2
y <- 0.1
x <- rnorm(100)
integrate(Q, lower=-10, upper=10, y=y)$value
crps(y, x)
crps(y, x, w=0.5)
crps(y, x, w=0.1)
document()
?crps
F1 <- function(x) pnorm(x)
I1 <- function(x, y) as.numeric(x >= y)
Q <- function(x, y) (F1(x) - I1(x, y))^2
y <- 0.1
x <- rnorm(100)
integrate(Q, lower=-10, upper=10, y=y)$value
crps(y, x)
crps(y, x, w=0.5)
crps(y, x, w=1)
document()
library(quack)
?glasso
library(quack)
?glasso
document()
devtools::document()
?glasso
library(devtools)
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
library(glasso)
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
document()
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
plot(fit)
fit$vars
fit$KIC
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
KIC <- rep(NA, K_trunc)
KIC[1] <- Inf
Caa <- diag(c((md+mo-1)*mo^2, (A_deg + A_ord -1)*A_ord^2))
Caa_inv <- diag(1/diag(Caa))
best <- list(k=0, KIC=Inf, map=NULL)
k
k=2
map_k <- estimate_map(y, phi[,1:k,drop=FALSE], Caa_inv[1:k, 1:k, drop=FALSE])
yhat_k <- phi[,1:k]%*%map_k$a
KIC[k] <- -2*sum(dnorm(y, yhat_k, map_k$sig, log=TRUE)) -
2*sum(dnorm(map_k$a, 0, sqrt(diag(Caa)), log=TRUE)) -
lambda*(k+1)*log(2*pi) + log(det(map_k$Caa_inv))
if(KIC[k] <= best$KIC){
best$k   <- k
best$KIC <- KIC[k]
best$map <- map_k
}
KIC[2]
KIC[1]
k=3
map_k <- estimate_map(y, phi[,1:k,drop=FALSE], Caa_inv[1:k, 1:k, drop=FALSE])
yhat_k <- phi[,1:k]%*%map_k$a
KIC[k] <- -2*sum(dnorm(y, yhat_k, map_k$sig, log=TRUE)) -
2*sum(dnorm(map_k$a, 0, sqrt(diag(Caa)), log=TRUE)) -
lambda*(k+1)*log(2*pi) + log(det(map_k$Caa_inv))
KIC[3]
k = 8
KIC <- rep(NA, K_trunc)
KIC[1] <- Inf
Caa <- diag(c((md+mo-1)*mo^2, (A_deg + A_ord -1)*A_ord^2))
Caa_inv <- diag(1/diag(Caa))
best <- list(k=0, KIC=Inf, map=NULL)
for(k in 2:K_trunc){
map_k <- estimate_map(y, phi[,1:k,drop=FALSE], Caa_inv[1:k, 1:k, drop=FALSE])
yhat_k <- phi[,1:k]%*%map_k$a
KIC[k] <- -2*sum(dnorm(y, yhat_k, map_k$sig, log=TRUE)) -
2*sum(dnorm(map_k$a, 0, sqrt(diag(Caa)), log=TRUE)) -
lambda*(k+1)*log(2*pi) + log(det(map_k$Caa_inv))
if(KIC[k] <= best$KIC){
best$k   <- k
best$KIC <- KIC[k]
best$map <- map_k
}
}
k
k= 8
map_k <- estimate_map(y, phi[,1:k,drop=FALSE], Caa_inv[1:k, 1:k, drop=FALSE])
yhat_k <- phi[,1:k]%*%map_k$a
KIC[k] <- -2*sum(dnorm(y, yhat_k, map_k$sig, log=TRUE)) -
2*sum(dnorm(map_k$a, 0, sqrt(diag(Caa)), log=TRUE)) -
lambda*(k+1)*log(2*pi) + log(det(map_k$Caa_inv))
KIC[8]
KIC
k =2
map_k <- estimate_map(y, phi[,1:k,drop=FALSE], Caa_inv[1:k, 1:k, drop=FALSE])
yhat_k <- phi[,1:k]%*%map_k$a
KIC[k] <- -2*sum(dnorm(y, yhat_k, map_k$sig, log=TRUE)) -
2*sum(dnorm(map_k$a, 0, sqrt(diag(Caa)), log=TRUE)) -
lambda*(k+1)*log(2*pi) + log(det(map_k$Caa_inv))
KIC
k=3
pih
phi
md = 8
mo = 4
A_set <- generate_A(p, md, mo)
A_deg <- apply(A_set, 1, sum)
A_ord <- apply(A_set, 1, function(aa) sum(aa > 0))
A_set <- generate_A(p, md, mo)
p = 5
A_set <- generate_A(p, md, mo)
A_deg <- apply(A_set, 1, sum)
A_ord <- apply(A_set, 1, function(aa) sum(aa > 0))
N_alpha <- nrow(A_set)
if(verbose) cat("\tFound ", N_alpha, " combinations.\n", sep="")
phi <- matrix(NA, nrow=n, ncol=N_alpha)
rr <- rep(NA, N_alpha)
n = nrow(X)
dim(X)
p = 4
A_set <- generate_A(p, md, mo)
A_deg <- apply(A_set, 1, sum)
A_ord <- apply(A_set, 1, function(aa) sum(aa > 0))
N_alpha <- nrow(A_set)
if(verbose) cat("\tFound ", N_alpha, " combinations.\n", sep="")
phi <- matrix(NA, nrow=n, ncol=N_alpha)
rr <- rep(NA, N_alpha)
for(i in 1:N_alpha){
curr <- rep(1, n)
for(j in 1:p){
curr <- curr * ss_legendre_poly(X[,j], A_set[i,j])
}
phi[,i] <- curr
rr[i] <- cor(curr, y)
}
dim(phi)
plot(rr)
ord <- rev(order(rr^2))
plot(rr[ord])
plot(rr[ord]^2)
phi
dim(phi)
length(y)
lasso_fit <- glmnet::glmnet(phi, y)
?glmnet
rr^2
plot(rr^2)
range(rr^2)
lasso_fit <- glmnet::glmnet(phi, y, weights=rr^2)
dim(phi)
1 - rr^2
range(1 - rr^2)
lasso_fit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2)
lasso_fit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n)
lasso_fit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
lasso_fit
lasso_fit$a0
class(lasso_fit$a0)
mean(y)
sd(y)
mu_y <- mean(y)
sig_y <- sd(y)
y <- (y - mu_y)/sig_y
lasso_fit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
lasso_fit$a0
lasso_fit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
foo = lasso_fit$beta
class(foo)
dim(foo)
apply(foo, 1, function(bb) sum(bb != 0))
apply(foo, 2, function(bb) sum(bb != 0))
apply(foo, 2, function(bb) sum(bb != 0))
apply(lasso_fit$beta, 2, function(bb) sum(bb != 0))
### LASSO IF NEEDED?
lfit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
count_nonzero <- apply(lfit$beta, 2, function(bb) sum(bb != 0))
count_nonzero
n
N_alpha
length(rr)
lfit$lambda
1/n
1/N_alpha
1/N_alpha^2
### LASSO IF NEEDED?
lfit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
lfit$lambda
md
md = 10
p
p = 5
X <- rbind(X, runif(300))
dim(X)
X <- lhs::maximinLHS(300, 5)
n <- nrow(X)
p <- ncol(X)
mu_y <- mean(y)
sig_y <- sd(y)
y <- (y - mu_y)/sig_y
md
mo
mo=5
A_set <- generate_A(p, md, mo)
A_deg <- apply(A_set, 1, sum)
A_ord <- apply(A_set, 1, function(aa) sum(aa > 0))
N_alpha <- nrow(A_set)
if(verbose) cat("\tFound ", N_alpha, " combinations.\n", sep="")
phi <- matrix(NA, nrow=n, ncol=N_alpha)
rr <- rep(NA, N_alpha)
for(i in 1:N_alpha){
curr <- rep(1, n)
for(j in 1:p){
curr <- curr * ss_legendre_poly(X[,j], A_set[i,j])
}
phi[,i] <- curr
rr[i] <- cor(curr, y)
}
ord <- rev(order(rr^2))
A_set <- A_set[ord,]
A_deg <- A_deg[ord]
A_ord <- A_ord[ord]
phi <- phi[,ord]
rr <- rr[ord]
plot(rr^2)
### LASSO IF NEEDED?
lfit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
count_nonzero <- apply(lfit$beta, 2, function(bb) sum(bb != 0))
count_nonzero
N_alpha
max(which(count_nonzero < n))
lambda_indx_use <- max(which(count_nonzero < n))
indx_use <- max(which(count_nonzero < n))
lambda_use <- lfit$lambda[indx_use]
lambda_use
lfit$beta[indx_use,]
dim(lfit_beta)
dim(lfit$beta)
lfit$beta[,indx_use]
length(lfit$beta[,indx_use])
N_alpha
which(lfit$beta[,indx_use] > 0)
lambda_indx <- max(which(count_nonzero < n))
lambda_use <- lfit$lambda[lambda_indx]
which(lfit$beta[,indx_use] > 0)
alpha_indx <- which(lfit$beta[,lambda_indx] > 0)
alpha_indx
which(1:5 > 10)
alpha_indx <- which(lfit$beta[,lambda_indx] > 0)
if(length(alpha_indx) == 0) alpha_indx <- 1
A_set <- A_set[alpha_indx,]
A_deg <- A_deg[alpha_indx]
A_ord <- A_ord[alpha_indx]
phi <- phi[,alpha_indx]
rr <- rr[alpha_indx]
A_set
dim(A_set)
length(alpha_indx)
max(which(count_nonzero < n))
count_nonzero
n
length(lfit$lambda)
sum(lfit$beta[,lambda_indx] != 0
)
alpha_indx <- which(lfit$beta[,lambda_indx] != 0)
length(alpha_indx)
if(length(alpha_indx) == 0) alpha_indx <- 1
A_set <- A_set[alpha_indx,]
A_deg <- A_deg[alpha_indx]
A_ord <- A_ord[alpha_indx]
A_set <- generate_A(p, md, mo)
A_deg <- apply(A_set, 1, sum)
A_ord <- apply(A_set, 1, function(aa) sum(aa > 0))
N_alpha <- nrow(A_set)
if(verbose) cat("\tFound ", N_alpha, " combinations.\n", sep="")
phi <- matrix(NA, nrow=n, ncol=N_alpha)
rr <- rep(NA, N_alpha)
for(i in 1:N_alpha){
curr <- rep(1, n)
for(j in 1:p){
curr <- curr * ss_legendre_poly(X[,j], A_set[i,j])
}
phi[,i] <- curr
rr[i] <- cor(curr, y)
}
ord <- rev(order(rr^2))
A_set <- A_set[ord,]
A_deg <- A_deg[ord]
A_ord <- A_ord[ord]
phi <- phi[,ord]
rr <- rr[ord]
### LASSO IF NEEDED?
lfit <- glmnet::glmnet(phi, y, penalty.factor=1-rr^2, dfmax=n-2)
count_nonzero <- apply(lfit$beta, 2, function(bb) sum(bb != 0))
lambda_indx <- max(which(count_nonzero < n))
lambda_use <- lfit$lambda[lambda_indx]
alpha_indx <- which(lfit$beta[,lambda_indx] != 0)
if(length(alpha_indx) == 0) alpha_indx <- 1
A_set <- A_set[alpha_indx,]
A_deg <- A_deg[alpha_indx]
A_ord <- A_ord[alpha_indx]
phi <- phi[,alpha_indx]
rr <- rr[alpha_indx]
plot(rr^2)
abline(h=0.001)
alpha_indx
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
document()
document()
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.02/8)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
plot(fit)
X <- lhs::maximinLHS(300, 4)
apply(X, 1, duqling::dms_additive)
var(apply(X, 1, duqling::dms_additive))
X <- lhs::maximinLHS(300, 4)
y <- apply(X, 1, duqling::dms_additive) + rnorm(300, 0, 0.2)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 8, max_order_init = 4)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 4, max_order_init = 2)
plot(fit)
?
duqling::piston
quack(piston)
quack("piston")
duqling::quack("piston")
duqling::quack()
X <- lhs::maximinLHS(300, 10)
y <- apply(X, 1, duqling::piston, scale01=TRUE) + rnorm(300, 0, 0.02)
var(y)
apply(X, 1, duqling::piston, scale01=TRUE)
var(apply(X, 1, duqling::piston, scale01=TRUE))
sd(apply(X, 1, duqling::piston, scale01=TRUE))
X <- lhs::maximinLHS(300, 10)
y <- apply(X, 1, duqling::piston, scale01=TRUE) + rnorm(300, 0, 0.01)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 4, max_order_init = 2)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 4, max_order_init = 2)
plot(fit)
X <- lhs::maximinLHS(300, 10)
y <- apply(X, 1, duqling::piston, scale01=TRUE) + rnorm(300, 0, 0.01)
fit <- bayes_chaos(X, y, max_degree = 8, max_degree_init = 4, max_order_init = 2)
plot(fit)
document()
document()
library(quack)
document()
library(quack)
document()
library(quack)
document()
